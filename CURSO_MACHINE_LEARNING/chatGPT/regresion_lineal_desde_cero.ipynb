{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Regresión lineal **desde cero** con `numpy` y `matplotlib`\n\n**Objetivo:** construir paso a paso modelos de regresión lineal sin usar librerías de alto nivel (nada de scikit-learn), entendiendo:\n\n- Generación de datos sintéticos\n- Función de coste MSE (Error Cuadrático Medio)\n- Gradientes y Descenso de Gradiente (batch, mini-batch, estocástico)\n- Vectorización con `numpy` y por qué acelera el código\n- División train/valid y estandarización (feature scaling)\n- Regresión lineal **multivariable** (con término de sesgo/bias)\n- **Ecuación normal** (solución cerrada) y `np.linalg.pinv`\n- **Regularización L2 (Ridge)** desde cero\n- Métricas: MSE, MAE, RMSE, R²\n- Curvas de pérdida y visualización con `matplotlib`\n\n> Todo viene hiper comentado. Podés correr las celdas una por una y jugar con los hiperparámetros.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) ¿Qué hacen `numpy` y `matplotlib` en este cuaderno?\n\n- **`numpy`**: librería para computación numérica rápida. Ideas clave que vamos a usar:\n  - `np.array([...])`: crea arreglos (vectores/matrices) eficientes.\n  - **Broadcasting**: `numpy` puede extender dimensiones automáticamente para hacer operaciones elemento a elemento sin bucles explícitos.\n  - `np.mean`, `np.sum`, `np.std`: estadísticas básicas eficientes.\n  - `np.dot(A, B)` o `A @ B`: producto matricial/vectores.\n  - `np.random`: generador de números aleatorios (por ej. ruido para simular datos reales).\n  - `np.c_`/`np.hstack`: concatenar columnas (útil para agregar la columna de 1s del **bias**).\n  - `np.linalg.pinv`: pseudoinversa (solución estable para la ecuación normal).\n\n- **`matplotlib`**: librería para graficar.\n  - `plt.scatter(x, y)`: puntos.\n  - `plt.plot(x, y)`: líneas.\n  - `plt.figure()`, `plt.title()`, `plt.xlabel()`, `plt.ylabel()`, `plt.legend()`: composición de la figura.\n\n> Consejo: corré cada celda y mirá salidas/errores. Cambiá parámetros y volvé a ejecutar.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Librerías base\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hacer reproducibles los resultados\nnp.random.seed(42)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Generemos un dataset sintético\nCreamos una relación aproximadamente lineal con ruido. Así imitamos datos del mundo real.\n\n**Parámetros a jugar:**\n- `n`: cantidad de muestras\n- `true_w`, `true_b`: parámetros \"reales\" de la relación\n- `noise_std`: cuánta dispersión/ruido tienen los datos\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Parámetros del dataset\nn = 200\ntrue_w = 2.5   # pendiente real\ntrue_b = -1.0  # bias/intersección real\nnoise_std = 1.5\n\n# Generamos X como 1 variable (x1) entre -3 y 3\nX = np.linspace(-3, 3, n).reshape(-1, 1)  # shape (n, 1)\n# y = w*x + b + ruido\ny = true_w * X[:, 0] + true_b + np.random.normal(loc=0.0, scale=noise_std, size=n)  # vector shape (n,)\n\n# Visualicemos\nplt.figure()\nplt.scatter(X[:,0], y, label=\"Datos (x, y)\")\nplt.title(\"Datos sintéticos con ruido\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Split: train / valid y (opcional) test\nSeparar datos para entrenar y evaluar generalización. Acá hacemos un split simple 80/20.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Índices y shuffle\nidx = np.arange(n)\nnp.random.shuffle(idx)\n\ntrain_frac = 0.8\nn_train = int(n * train_frac)\n\ntrain_idx = idx[:n_train]\nvalid_idx = idx[n_train:]\n\nX_train, y_train = X[train_idx], y[train_idx]\nX_valid, y_valid = X[valid_idx], y[valid_idx]\n\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Escalado de características (estandarización)\n\nEl descenso de gradiente suele converger mejor si las features están en una escala comparable.\nEstandarizamos cada columna: `X_std = (X - mean) / std`.\nGuardamos `mean` y `std` del **train** y los reutilizamos en valid/test.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def standardize_fit(X):\n    \"\"\"Calcula media y std por columna y devuelve (X_std, mean, std).\"\"\"\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0) + 1e-8  # evitamos división por 0\n    X_std = (X - mean) / std\n    return X_std, mean, std\n\ndef standardize_apply(X, mean, std):\n    return (X - mean) / std\n\nX_train_std, mean_, std_ = standardize_fit(X_train)\nX_valid_std = standardize_apply(X_valid, mean_, std_)\n\nX_train_std[:3], mean_, std_\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Agregar el **bias** como columna de 1s\n\nPara trabajar de forma vectorizada, armamos la matriz de diseño `Φ` (phi):\n\n```\nΦ = [1, x1]\n```\n\nAsí los parámetros se agrupan en `w = [b, w1]` y la predicción es `y_hat = Φ @ w`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def add_bias(X):\n    \"\"\"Agrega una columna de 1s al frente.\"\"\"\n    return np.c_[np.ones((X.shape[0], 1)), X]\n\nPhi_train = add_bias(X_train_std)\nPhi_valid = add_bias(X_valid_std)\n\nPhi_train.shape, Phi_valid.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Métricas + Coste (MSE) y otras\n\n- **MSE**: `mean((y - y_hat)^2)` — lo usamos también como función de coste.\n- **MAE**: `mean(|y - y_hat|)`.\n- **RMSE**: `sqrt(MSE)` (en mismas unidades que y).\n- **R²**: 1 - SSE/SST (cuánto explica el modelo, 1 es perfecto, 0 es baseline de la media).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def predict(Phi, w):\n    return Phi @ w  # producto matricial\n\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mae(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mse(y_true, y_pred))\n\ndef r2_score(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - ss_res / (ss_tot + 1e-12)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Gradientes (vectorizado)\n\nPara MSE y predicción `y_hat = Φ @ w`:\n\n- Error: `e = y_hat - y`\n- Gradiente sin regularización: `∇w = (2/n) * Φᵀ @ e`\n\nCon **L2 (Ridge)** con λ:\n- Coste: `J = MSE + λ * ||w_no_bias||²`\n- Gradiente: `∇w = (2/n) * Φᵀ @ e + 2λ * w_no_bias` (sin penalizar el bias `w[0]`).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def compute_gradients(Phi, y, w, l2=0.0):\n    n = Phi.shape[0]\n    y_hat = predict(Phi, w)\n    e = y_hat - y\n    grad = (2.0 / n) * (Phi.T @ e)\n    if l2 > 0.0:\n        reg = 2.0 * l2 * w\n        reg[0] = 0.0  # no regularizamos el bias\n        grad = grad + reg\n    return grad, y_hat\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Descenso de Gradiente (batch / mini-batch / estocástico)\nParámetros:\n- `lr` (learning rate): tamaño del paso\n- `epochs`: cuántas pasadas sobre el dataset\n- `batch_size`: si `None` o `n`, es **batch**; si `1`, **estocástico**; intermedio: **mini-batch**\n- `l2`: regularización Ridge\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def gradient_descent(Phi, y, lr=0.1, epochs=200, batch_size=None, l2=0.0, shuffle=True, verbose=False):\n    n, d = Phi.shape\n    if batch_size is None:\n        batch_size = n  # batch completo\n    \n    # Inicializamos pesos a 0\n    w = np.zeros(d)\n    history = {\"epoch\": [], \"train_mse\": []}\n    \n    for epoch in range(1, epochs+1):\n        indices = np.arange(n)\n        if shuffle:\n            np.random.shuffle(indices)\n        \n        # iterar en minibatches\n        for start in range(0, n, batch_size):\n            end = start + batch_size\n            batch_idx = indices[start:end]\n            Phi_b = Phi[batch_idx]\n            y_b = y[batch_idx]\n            \n            grad, y_hat_b = compute_gradients(Phi_b, y_b, w, l2=l2)\n            w = w - lr * grad\n        \n        # Log de entrenamiento (en batch completo calculamos mse sobre todo train)\n        y_hat = predict(Phi, w)\n        train_mse = mse(y, y_hat)\n        history[\"epoch\"].append(epoch)\n        history[\"train_mse\"].append(train_mse)\n        \n        if verbose and (epoch % max(1, epochs//10) == 0):\n            print(f\"Epoch {epoch:4d} | MSE: {train_mse:.4f}\")\n    \n    return w, history\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Entrenemos y miremos curvas\nProbamos batch GD con distintos `lr`. También graficamos el ajuste final en 1D.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Entrenamiento (batch)\nw_gd, hist = gradient_descent(Phi_train, y_train, lr=0.1, epochs=300, batch_size=None, l2=0.0, verbose=True)\n\nprint(\"Pesos (w) [bias, w1]:\", w_gd)\nprint(\"MSE train:\", mse(y_train, predict(Phi_train, w_gd)))\nprint(\"MSE valid:\", mse(y_valid, predict(Phi_valid, w_gd)))\nprint(\"R2   valid:\", r2_score(y_valid, predict(Phi_valid, w_gd)))\n\n# Curva de pérdida\nplt.figure()\nplt.plot(hist[\"epoch\"], hist[\"train_mse\"], label=\"MSE train\")\nplt.title(\"Curva de entrenamiento (MSE)\")\nplt.xlabel(\"Época\")\nplt.ylabel(\"MSE\")\nplt.legend()\nplt.show()\n\n# Visualizar ajuste en 1D (recordá que entrenamos con X estandarizado)\n# Para dibujar la recta, generamos un rango en el espacio estandarizado y lo llevamos a x original para el scatter\nplt.figure()\nplt.scatter(X_train[:,0], y_train, label=\"Train\")\n# Generamos puntos uniformes en el espacio de X estandarizado\nxs_std = np.linspace(X_train_std.min(), X_train_std.max(), 100).reshape(-1,1)\nphi_xs = add_bias(xs_std)\nys_hat = predict(phi_xs, w_gd)\n\n# Para graficar contra x original, convertimos xs_std a escala original inversa:\nxs_orig = xs_std * std_ + mean_\nplt.plot(xs_orig[:,0], ys_hat, label=\"Recta (pred)\")\nplt.title(\"Ajuste 1D\")\nplt.xlabel(\"x (original)\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Ecuación normal (solución cerrada)\n\nPara minimizar MSE sin regularización:\n\n`w = (ΦᵀΦ)^(-1) Φᵀ y`  \n\nEs más estable usar **pseudoinversa**: `w = pinv(Φ) @ y`.\n\nCon **Ridge** (λ):\n\n`w = (ΦᵀΦ + λI*)^(-1) Φᵀ y`  \n\ndonde `I*` es identidad con 0 en la posición del bias (para no regularizarlo).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def normal_equation(Phi, y, l2=0.0):\n    if l2 == 0.0:\n        # Pseudoinversa estable\n        return np.linalg.pinv(Phi) @ y\n    # Ridge: (Phi^T Phi + lambda * I*)^{-1} Phi^T y\n    d = Phi.shape[1]\n    A = Phi.T @ Phi\n    I = np.eye(d)\n    I[0,0] = 0.0  # no penalizamos bias\n    A_reg = A + l2 * I\n    return np.linalg.inv(A_reg) @ Phi.T @ y\n\nw_ne = normal_equation(Phi_train, y_train, l2=0.0)\nprint(\"NE pesos:\", w_ne)\nprint(\"NE MSE valid:\", mse(y_valid, predict(Phi_valid, w_ne)))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Multivariable + Polinomios + Regularización\n\nAgreguemos más features (x², x³) para simular polinomios y apliquemos Ridge.\nCuidado con el **overfitting**: regularización ayuda a controlar la complejidad.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Construimos features polinomiales hasta grado 3 a partir de X (1D)\ndef poly_features(x, degree=3):\n    # x: shape (n,1) -> devuelve [x, x^2, ..., x^degree]\n    feats = [x]\n    for p in range(2, degree+1):\n        feats.append(x**p)\n    return np.hstack(feats)\n\nX_train_poly = poly_features(X_train_std, degree=3)\nX_valid_poly = poly_features(X_valid_std, degree=3)\n\nPhi_train_poly = add_bias(X_train_poly)\nPhi_valid_poly = add_bias(X_valid_poly)\n\n# Entrenar con Ridge para evitar sobreajuste\nw_poly, hist_poly = gradient_descent(Phi_train_poly, y_train, lr=0.05, epochs=500, batch_size=None, l2=0.1, verbose=False)\n\nprint(\"MSE train (poly+ridge):\", mse(y_train, predict(Phi_train_poly, w_poly)))\nprint(\"MSE valid (poly+ridge):\", mse(y_valid, predict(Phi_valid_poly, w_poly)))\nprint(\"R2   valid (poly+ridge):\", r2_score(y_valid, predict(Phi_valid_poly, w_poly)))\n\nplt.figure()\nplt.plot(hist_poly[\"epoch\"], hist_poly[\"train_mse\"], label=\"MSE train (poly)\")\nplt.title(\"Curva de entrenamiento (polinomios + Ridge)\")\nplt.xlabel(\"Época\")\nplt.ylabel(\"MSE\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) Para practicar (tareas sugeridas)\n1. Cambiá `noise_std` y observá cómo afecta MSE y R².\n2. Probá distintos `lr` (`0.001`, `0.01`, `0.1`, `0.3`) y explicá cuándo diverge.\n3. Usá `batch_size=1` (SGD) y `batch_size=16` (mini-batch). ¿Cambia la curva de MSE?\n4. Subí el grado del polinomio (`degree=5, 7, 10`). ¿Qué pasa sin regularización vs con `l2=1.0`?\n5. Implementá **early stopping**: guardá el mejor MSE en valid y pará si no mejora en 50 épocas.\n6. Agregá nuevas features manualmente (por ejemplo `sin(x)`) y compará.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}